{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de6f2fea",
   "metadata": {},
   "source": [
    "# Data Wrangling Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20d2ad3",
   "metadata": {},
   "source": [
    "## Data Formatting part 1\n",
    "\n",
    "### Correcting Datatypes\n",
    "Converting the datatype to correct format by exploring.\n",
    "- Identify the datatypes of each column\n",
    "    ```python\n",
    "    print(dataframe.dtypes)\n",
    "    ```\n",
    "- Converting necessary datatypes. We can use float, float32, float64, int, int32, int64, str. We can prevent the data loss by taking the higher precision datatype __64\n",
    "    ```python\n",
    "    dataframe[float_column_names] = dataframe[float_column_names].astype(\"float64\")\n",
    "    dataframe[int_column_names] = dataframe[int_column_names].astype(\"int64\")\n",
    "    dataframe[str_column_names] = dataframe[str_column_names].astype(\"str\")\n",
    "    print(dataframe.dtypes)\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc09bb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_datatype(df, float_column_names, int_column_names, str_column_names):\n",
    "    print(df.dtypes)\n",
    "    \n",
    "    df[float_column_names] = df[float_column_names].astype(\"float64\")\n",
    "    df[int_column_names] = df[int_column_names].astype(\"int64\")\n",
    "    df[str_column_names] = df[str_column_names].astype(\"str\")\n",
    "    print(df.dtypes)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10e1bdc",
   "metadata": {},
   "source": [
    "### Maintaining Data Consistency\n",
    "#### String Cleaning\n",
    "- strip() method of a string removes leading and trailing whitespace (spaces, tabs, and newline characters) from a string. You can specify the characters you want to remove by providing a regular expression to the method. The argument must contains all the characters that are not allowed in leading or trailing. It doesn't affect within the middle of the string. \n",
    "- lstrip() method works stripping on leading strings\n",
    "- rsrtip() method works on trailing strings\n",
    "    ```python\n",
    "    dataframe[column_names] = dataframe[column_names].str.strip(\"1234567890!@#$%^&*()_+=<>?,./;:'\\\"[]{}\\\\|-`~\")\n",
    "    dataframe[column_names] = dataframe[column_names].str.lstrip()\n",
    "    dataframe[column_names] = dataframe[column_names].str.rstrip()\n",
    "    ```\n",
    "- standarizing a string needs to first remove something or replace with '' empty string and then replace with standard form\n",
    "    ```python\n",
    "    # [^] means anything except [] to replace with empty \n",
    "    dataframe[column] = dataframe[column].str.replace('[^a-zA-Z0-9]', '')\n",
    "    \n",
    "    # standarizing Example\n",
    "    dataframe[column] = dataframe[column].apply(lambda x: str(x))\n",
    "    dataframe[column] = dataframe[column].apply(lambda x: x[0:3] + '-' + x[3:6] + '-' + x[6:10])\n",
    "    ```\n",
    "- Standarizing depends on us completely. We have to carefully do everything\n",
    "\n",
    "- We can split a value into columns if necessary\n",
    "    ```python\n",
    "    # 1st argument is to tell when to split, 2nd is the number of splits\n",
    "    dataframe[col_names] = dataframe[col_to_split].str.split(',',2, expand=True)\n",
    "    ```\n",
    "- We can correct typos or replace withing string\n",
    "\n",
    "- Character encodings are specific sets of rules for mapping from raw binary byte strings (that look like this: 0110100001101001) to characters that make up human-readable text (like \"hi\"). You might also end up with a \"unknown\" characters. There are what gets printed when there's no mapping between a particular byte and a character in the encoding you're using to read your byte string in and looking like ???.. UTF-8 is the standard text encoding. All Python code is in UTF-8 and, ideally, all your data should be as well. It's when things aren't in UTF-8 that you run into trouble. If you look at a bytes object, you'll see that it has a b in front of it, and then maybe some text after. That's because bytes are printed out as if they were characters encoded in ASCII. \n",
    "It was pretty hard to deal with encodings in Python 2, but thankfully in Python 3 it's a lot simpler. (Kaggle Notebooks only use Python 3.) There are two main data types you'll encounter when working with text in Python 3. One is is the string, other one is bytes.\n",
    "however, any characters not in ASCII will just be replaced with the unknown character. Then, when we convert the bytes back to a string, the character will be replaced with the unknown character. The dangerous part about this is that there's not way to tell which character it should have been. That means we may have just made our data unusable!\n",
    "    ```python\n",
    "    # start with a string\n",
    "    before = \"This is the euro symbol: â‚¬\"\n",
    "\n",
    "    # encode it to a different encoding, replacing characters that raise errors\n",
    "    after = before.encode(\"ascii\", errors = \"replace\")\n",
    "\n",
    "    # convert it back to utf-8\n",
    "    print(after.decode(\"ascii\"))\n",
    "\n",
    "    # This is the euro symbol: ?\n",
    "    \n",
    "    new_entry = (sample_entry.decode(\"ASCII\", errors='replace')).encode(\"utf-8\", errors='replace')\n",
    "    ```\n",
    "- Reading files with encoding problems: During reading a csv a file we may encounter UnicodeDecodeError. This tells us that this file isn't actually UTF-8. We don't know what encoding it actually is though. \n",
    "1. One way to figure it out is to try and test a bunch of different character encodings and see if any of them work. \n",
    "2. A better way, though, is to use the charset_normalizer module to try and automatically guess what the right encoding is. It's not 100% guaranteed to be right, but it's usually faster than just trying to guess. \n",
    "3. just look at the first ten thousand bytes of this file. This is usually enough for a good guess about what the encoding is and is much faster than trying to look at the whole file. \n",
    "\n",
    "    ```python\n",
    "    # helpful character encoding module\n",
    "    import charset_normalizer\n",
    "    \n",
    "    # try to read in a file not in UTF-8\n",
    "    df = pd.read_csv(\"abcd.csv\")\n",
    "    \n",
    "    # Error: UnicodeDecodeError                        Traceback (most recent call last)\n",
    "    \n",
    "    # look at the first ten thousand bytes to guess the character encoding\n",
    "    with open(\"abcd.csv\", 'rb') as rawdata:\n",
    "        result = charset_normalizer.detect(rawdata.read(10000))\n",
    "\n",
    "    # check what the character encoding might be\n",
    "    print(result)\n",
    "    \n",
    "    # {'encoding': 'Windows-1252', 'language': 'English', 'confidence': 0.73} \n",
    "    # So charset_normalizer is 73% confidence that the right encoding is \"Windows-1252\". Let's see if that's correct:\n",
    "    \n",
    "    # read in the file with the encoding detected by charset_normalizer\n",
    "    df = pd.read_csv(\"abcd.csv\", encoding='Windows-1252')\n",
    "\n",
    "    # look at the first few lines\n",
    "    df.head()\n",
    "    ```\n",
    "    \n",
    "- Case Standarizing: Converting every string on upper or lower case\n",
    "    ```python\n",
    "    dataframe[column] = dataframe[column].str.lower()\n",
    "    dataframe[column] = dataframe[column].str.upper()\n",
    "    ```\n",
    "- Removing non Alphanumeric characters\n",
    "    ```python\n",
    "    dataframe[column] = dataframe[column].str.replace('[^a-zA-Z0-9\\s]', '', regex=True)\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ada4de",
   "metadata": {},
   "source": [
    "#### Date and Time Cleaning\n",
    "- Making sure that all your dates and times are either a DateTime object or a Unix timestamp (via type coercion). No strings pretending to be a DateTime object\n",
    "    ```python\n",
    "    # Check the column's type\n",
    "    print(dataframe[datetime_column].head())\n",
    "    #  bottom of the output of head(), you can see that it says that the data type of this column is \"object\".\n",
    "    ```\n",
    "- We should check the date column's string length to confirm length is same. Output of the code indicates length and then the number of occurance of that length\n",
    "    ```python\n",
    "    date_lengths = dataframe.column_name.str.len()\n",
    "    print(date_lengths.value_counts())\n",
    "    ```\n",
    "- If there is different length, We can retrieve the indices of those irregularities and check the records\n",
    "    ```python\n",
    "    indices = np.where([date_lengths == len_with_irr])[1]\n",
    "    print('Indices with corrupted data:', indices)\n",
    "    print(dataframe.loc[indices])\n",
    "    ```\n",
    "- For those indices we can manually change the format to given format\n",
    "    ```python\n",
    "    for i in indices:\n",
    "        dataframe.loc[i, 'Date'] = parser.parse(dataframe.loc[i, 'Date'])\n",
    "        dataframe.loc[i, 'Date'] = dataframe.loc[i, 'Date'].strftime(\"%m/%d/%Y\")\n",
    "    ```\n",
    "- convert the date columns to datetime. Check python format in [here](https://strftime.org/). There are lots of possible parts of a date, but the most common are %d for day, %m for month, %y for a two-digit year and %Y for a four digit year. Some examples:\n",
    "    - 1/17/07 has the format \"%m/%d/%y\"\n",
    "    - 17-1-2007 has the format \"%d-%m-%Y\"\n",
    "    ```python\n",
    "    # create a new column, date_parsed, with the parsed dates\n",
    "    from datetime import datetime\n",
    "    dataframe['date_parsed'] = pd.to_datetime(dataframe[datetime_column], format=\"%m/%d/%y\")\n",
    "    # the dtype is datetime64 if we see head\n",
    "    ```\n",
    "- Problems with multiple date formats: If we see an error when there are multiple date formats in a single column, you can have pandas try to infer what the right date format should be. \n",
    "    ```python\n",
    "    dataframe['date_parsed'] = pd.to_datetime(dataframe[datetime_column], infer_datetime_format=True)\n",
    "    ```\n",
    "There are two big reasons not to always have pandas guess the time format. The first is that pandas won't always been able to figure out the correct date format, especially if someone has gotten creative with data entry. The second is that it's much slower than specifying the exact format of the dates.\n",
    "- Select the day of the month\n",
    "    ```python\n",
    "    dataframe['date_parsed'].dt.day\n",
    "    ```\n",
    "- Plot the day: One of the biggest dangers in parsing dates is mixing up the months and days. Hence double checking is needed. To do this, let's plot a histogram of the days of the month. We expect it to have values between 1 and 31\n",
    "    ```python\n",
    "    import seaborn as sns\n",
    "    sns.distplot(dataframe['date_parsed'].dt.day, kde=False, bins=31)\n",
    "    ```\n",
    "- DateTime objects are often recorded with or without time zones, which can cause issues. If you are doing region-specific analysis, ensure that DateTime objects are in the correct time zone. If internationalization is not a concern, consider converting all DateTime objects to a specific time zone.\n",
    "    ```python\n",
    "    import pytz  # For time zone handling\n",
    "    dataframe['date_time'] = dataframe['date_time'].apply(lambda x: datetime.fromtimestamp(x, pytz.timezone('US/Eastern')))\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c67a420",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import seaborn as sns\n",
    "import pytz  # For time zone handling\n",
    "\n",
    "def cleaning_datetime(df, col):\n",
    "    # Check the column's type\n",
    "    print(df[col].head())\n",
    "    \n",
    "    # Check length with frequency\n",
    "    date_lengths = df.col.str.len()\n",
    "    print(date_lengths.value_counts())\n",
    "    \n",
    "    # Check irregular length record here\n",
    "    indices = np.where([date_lengths == len_with_irr])[1]\n",
    "    print('Indices with corrupted data:', indices)\n",
    "    print(df.loc[indices])\n",
    "    \n",
    "    # Correct irregular length record here\n",
    "    for i in indices:\n",
    "        df.loc[i, col] = parser.parse(df.loc[i, col])\n",
    "        df.loc[i, col] = df.loc[i, col].strftime(\"%m/%d/%Y\")\n",
    "        \n",
    "    # Change format\n",
    "    df['date_parsed'] = pd.to_datetime(df[col], format=\"%m/%d/%y\")\n",
    "    \n",
    "    # If multiple format then write this line instead\n",
    "    df['date_parsed'] = pd.to_datetime(df[col], infer_datetime_format=True)\n",
    "    \n",
    "    # Check the column's type\n",
    "    print(df[col].head())\n",
    "    \n",
    "    # Check days by plot if there in 1-30\n",
    "    sns.distplot(df['date_parsed'].dt.day, kde=False, bins=31)\n",
    "    \n",
    "    # Change zone\n",
    "    df['date_time'] = df['date_time'].apply(lambda x: datetime.fromtimestamp(x, pytz.timezone('US/Eastern')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb799bd9",
   "metadata": {},
   "source": [
    "#### Fix Inconsistent Data Entries\n",
    "- To get closer look of a feature we can view row\n",
    "    ```python\n",
    "    countries = professors['Country'].unique()\n",
    "    ```\n",
    "- sort them alphabetically and then take a closer look\n",
    "    ```python\n",
    "    countries.sort()\n",
    "    countries\n",
    "    ```\n",
    "- Handle the upper lower consistencies\n",
    "- There could be same entry but with different style example: southkorea south korea. use the fuzzywuzzy package to help identify which strings are closest to each other.\n",
    "Fuzzywuzzy returns a ratio given two strings. The closer the ratio is to 100, the smaller the edit distance between the two strings. Here, we're going to get the ten strings from our list of cities that have the closest distance to \"south korea\".\n",
    "    ```python\n",
    "    # get the top 10 closest matches to \"south korea\"\n",
    "    matches = fuzzywuzzy.process.extract(\"south korea\", countries, limit=10, scorer=fuzzywuzzy.fuzz.token_sort_ratio)\n",
    "\n",
    "    # take a look at them\n",
    "    matches\n",
    "    ```\n",
    "output:\n",
    "[('south korea', 100),\n",
    " ('southkorea', 48),\n",
    " ('saudi arabia', 43),\n",
    " ('norway', 35),\n",
    " ('austria', 33),\n",
    " ('ireland', 33),\n",
    " ('pakistan', 32),\n",
    " ('portugal', 32),\n",
    " ('scotland', 32),\n",
    " ('australia', 30)]\n",
    " \n",
    " We can see that two of the items in the cities are very close to \"south korea\": \"south korea\" and \"southkorea\". Let's replace all rows in our \"Country\" column that have a ratio of > 47 with \"south korea\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24cb54c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to replace rows in the provided column of the provided dataframe\n",
    "# that match the provided string above the provided ratio with the provided string\n",
    "def replace_matches_in_column(df, column, string_to_match, min_ratio = 47):\n",
    "    # get a list of unique strings\n",
    "    strings = df[column].unique()\n",
    "    \n",
    "    # get the top 10 closest matches to our input string\n",
    "    matches = fuzzywuzzy.process.extract(string_to_match, strings, \n",
    "                                         limit=10, scorer=fuzzywuzzy.fuzz.token_sort_ratio)\n",
    "\n",
    "    # only get matches with a ratio > 90\n",
    "    close_matches = [matches[0] for matches in matches if matches[1] >= min_ratio]\n",
    "\n",
    "    # get the rows of all the close matches in our dataframe\n",
    "    rows_with_matches = df[column].isin(close_matches)\n",
    "\n",
    "    # replace all rows with close matches with the input matches \n",
    "    df.loc[rows_with_matches, column] = string_to_match\n",
    "\n",
    "    \n",
    "    # let us know the function's done\n",
    "    print(\"All done!\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b37258",
   "metadata": {},
   "source": [
    "## Duplicate Data Detection and Treatment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71cc3a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your dataset (replace 'your_data.csv' with your actual file)\n",
    "# df = pd.read_csv('your_data.csv')\n",
    "\n",
    "# Function to detect duplicate rows\n",
    "def detect_duplicates(dataframe):\n",
    "    duplicate_rows = dataframe[dataframe.duplicated()]\n",
    "    return duplicate_rows\n",
    "\n",
    "# Function to remove duplicate rows\n",
    "def remove_duplicates(dataframe):\n",
    "    dataframe_no_duplicates = dataframe.drop_duplicates()\n",
    "    return dataframe_no_duplicates\n",
    "\n",
    "# Example usage\n",
    "# Display duplicate rows\n",
    "duplicate_rows = detect_duplicates(df)\n",
    "print(\"Duplicate Rows:\")\n",
    "print(duplicate_rows)\n",
    "\n",
    "# Remove duplicate rows\n",
    "df_no_duplicates = remove_duplicates(df)\n",
    "print(\"\\nDataFrame after Removing Duplicates:\")\n",
    "print(df_no_duplicates)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
