{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf074e34",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15297472",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis and Data Cleaning\n",
    "\n",
    "### Variable Identification\n",
    "1. Identify Predictor and Target variable\n",
    "```python\n",
    "predictor_variables = df.drop(columns=[useless_column_names])\n",
    "target_variable = df['target_variable_name']\n",
    "\n",
    "print(predictor_variables.columns)\n",
    "print(target_variable.head())\n",
    "```\n",
    "2. Identify the data type and category of the variables.\n",
    "- We can learn the shape of object types of our data.\n",
    "- We cannot identify it with code. It's better to identify with our knowledge by seeing the data.\n",
    "    - Read the problem and identify the variables described. Note key properties of the variables, such as what types of values the variables can take.\n",
    "    - Identify any variables from step 1 that take on values from a limited number of possible values with no particular ordering. These variables are categorical.\n",
    "\n",
    "3. Dividing them to subcategories for further analysis\n",
    "https://towardsdatascience.com/data-types-in-statistics-347e152e8bee\n",
    "\n",
    "#### Categorical Data:\n",
    "\n",
    "1. **Nominal Data:**\n",
    "   - Definition: Categories without any inherent order or ranking.\n",
    "   - Examples: Colors, gender, types of animals.\n",
    "   - Identification: Check if the data consists of distinct categories without any implied order.\n",
    "   - use one hot encoding, to transform nominal data into a numeric feature.\n",
    "   - **Visualization Tool:** Bar Chart or Pie Chart.\n",
    "     - Use different colors or patterns for each category.\n",
    "     - Find frequencies, percentage and proportion by dividing the frequency by the total number of events. (e.g how often something happened divided by how often it could happen)\n",
    "\n",
    "2. **Ordinal Data:**\n",
    "   - Definition: Categories with a clear order or ranking.\n",
    "   - Examples: Education levels, customer satisfaction ratings.\n",
    "   - Identification: Look for data where the categories have a meaningful sequence or hierarchy.\n",
    "   - use one label encoding, to transform ordinal data into a numeric feature.\n",
    "   - - **Visualization Tool:** Ordered Bar Chart or Dot Plot or Pie Chart.\n",
    "     - Display the categories in a meaningful order.\n",
    "     - summarise frequencies, percentage and proportion by dividing the frequency by the total number of events. (e.g how often something happened divided by how often it could happen)\n",
    "     - use percentiles, median, mode and the interquartile range to summarise your data.\n",
    "\n",
    "#### Numerical Data:\n",
    "\n",
    "1. **Discrete Data:**\n",
    "   - Definition: Countable and distinct values.\n",
    "   - Examples: Number of cars, number of people.\n",
    "   - Identification: Data points are individual and separate; there are no fractional values.\n",
    "   - **Visualization Tool:** Histogram or Bar Chart.\n",
    "     - Display the count of each discrete value.\n",
    "     - summarise frequencies, percentage and proportion by dividing the frequency by the total number of events. (e.g how often something happened divided by how often it could happen)\n",
    "     - use mean, percentiles, median, mode and the interquartile range to summarise your data.\n",
    "\n",
    "2. **Continuous Data:**\n",
    "   - Definition: Infinite possible values within a given range or measurable but not countable.\n",
    "   - Examples: Height, weight, temperature.\n",
    "   - Identification: Data points can take any value within a range, including fractional values.\n",
    "   - **Visualization Tool:** Histogram or Box Plot.\n",
    "     - Show the distribution of data and identify outliers.\n",
    "     - Summarize percentiles, median, interquartile range, mean, mode, standard deviation, and range.\n",
    "     - With a histogram, you can check the central tendency, variability, modality, and kurtosis of a distribution. Note that a histogram can’t show you if you have any outliers. This is why we also use box-plots.\n",
    "     https://medium.com/@agarwal.vishal819/outlier-detection-with-boxplots-1b6757fafa21\n",
    "\n",
    "   - **Interval Data:**\n",
    "      - Definition: Data with a consistent interval between values, but no true zero point.\n",
    "      - Examples: Temperature in Celsius, IQ scores.\n",
    "      - Identification: Differences between values are meaningful, but ratios are not.\n",
    "      - **Visualization Tool:** Histogram or Line Chart.\n",
    "       - Highlight trends over a continuous range.\n",
    "\n",
    "   - **Ratio Data:**\n",
    "      - Definition: Data with a consistent interval between values and a true zero point.\n",
    "      - Examples: Height, weight, income.\n",
    "      - Identification: Ratios between values are meaningful (e.g., one value is twice or three times another).\n",
    "      - **Visualization Tool:** Scatter Plot or Box Plot.\n",
    "       - Explore relationships between variables and identify outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26311ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "nominal_columns = [\"column_name1\", \"column_name2\", ...]\n",
    "ordinal_columns = [\"column_name3\", \"column_name4\", ...]\n",
    "discrete_columns = [\"column_name5\", \"column_name6\", ...]\n",
    "interval_columns = [\"column_name7\", \"column_name8\", ...]\n",
    "ratio_columns = [\"column_name9\", \"column_name10\", ...]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a476be2",
   "metadata": {},
   "source": [
    "### Univariate Analysis\n",
    "https://medium.com/@agarwal.vishal819/outlier-detection-with-boxplots-1b6757fafa21\n",
    "https://www.analyticsvidhya.com/blog/2016/01/guide-data-exploration/\n",
    "https://github.com/krishnaik06/Machine-Learning-in-90-days/blob/master/Section%201-%20Python%20Crash%20Course/EDA.ipynb\n",
    "\n",
    "**Continuous Variable**\n",
    "- summary statistics, box plot and histogram\n",
    "- highlight missing and outlier values. \n",
    "\n",
    "**Categorical Variable**\n",
    "- Count and Count% against each category. \n",
    "- Bar chart can be used as visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44662d1b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_8307/641699332.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcolumn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcategorical_columns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0manalyze_categorical_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_8307/641699332.py\u001b[0m in \u001b[0;36manalyze_categorical_variable\u001b[0;34m(column_name)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0manalyze_categorical_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumn_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# Frequency table\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mfrequency_table\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcolumn_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# Percentage of values under each category\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function for univariate analysis of categorical variables\n",
    "def analyze_categorical_variable(column_name):\n",
    "    # Frequency table\n",
    "    frequency_table = df[column_name].value_counts()\n",
    "\n",
    "    # Percentage of values under each category\n",
    "    percentage_table = df[column_name].value_counts(normalize=True) * 100\n",
    "\n",
    "    # Combine count and percentage into a single DataFrame\n",
    "    summary_df = pd.DataFrame({'Count': frequency_table, 'Count%': percentage_table})\n",
    "\n",
    "    # Bar chart\n",
    "    summary_df.plot(kind='bar', y='Count', legend=False)\n",
    "    plt.title(f'Univariate Analysis - {column_name}')\n",
    "    plt.xlabel(column_name)\n",
    "    plt.ylabel('Count')\n",
    "    plt.show()\n",
    "\n",
    "# Analyze each categorical variable\n",
    "categorical_columns = ['Category_Column1', 'Category_Column2']\n",
    "\n",
    "for column in categorical_columns:\n",
    "    analyze_categorical_variable(column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17bd7d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Univariate\n",
    "# Get unique values in the 'targetcolumn' column\n",
    "unique_cols = df['targetcolumn'].unique()\n",
    "\n",
    "# Plotting loop\n",
    "for col in unique_cols:\n",
    "    data_variable = df.loc[df['targetcolumn'] == col]['col_name']\n",
    "    plt.plot(data_variable, np.zeros_like(data_variable), 'o', label=target)\n",
    "\n",
    "# Set labels and show the plot\n",
    "plt.xlabel('col name')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6208942e",
   "metadata": {},
   "source": [
    "### Bivariate Analysis\n",
    "- look for association and disassociation between variables at a pre-defined significance level.\n",
    "\n",
    "**Continuous & Continuous**\n",
    "- scatter plot\n",
    "- The relationship can be linear or non-linear. That helps to identify ML algos\n",
    "```python\n",
    "sns.FacetGrid(df,hue=\"target_variable\",size=5).map(plt.scatter,\"x_col\",\"y_col\").add_legend();\n",
    "plt.show()\n",
    "```\n",
    "If some points are not overlapped we can use linear.\n",
    "\n",
    "- To find the strength of the relationship, we use Correlation. Correlation varies between -1 and +1.\n",
    "-1: perfect negative linear correlation\n",
    "+1:perfect positive linear correlation and \n",
    "0: No correlation\n",
    "- Correlation = Covariance(X,Y) / SQRT( Var(X)* Var(Y))\n",
    "\n",
    "**Categorical & Categorical**\n",
    "- 2 methods\n",
    "1. **Two-way table:**\n",
    "    - two-way table of count and count%\n",
    "    - rows represents the category of one variable and the columns represent the categories of the other variable\n",
    "    - show count or count% of observations available in each combination of row and column categories.\n",
    "2. **Stacked Column Chart:** visual form of Two-way table\n",
    "3. **Chi-Square Test:** \n",
    "    - derive the statistical significance of relationship between the variables.\n",
    "    - tests whether the evidence in the sample is strong enough to generalize that the relationship for a larger population as well\n",
    "    - difference between the expected and observed frequencies in one or more categories in the two-way table. It returns probability for the computed chi-square distribution with the degree of freedom.\n",
    "Probability of 0: It indicates that both categorical variable are dependent\n",
    "\n",
    "Probability of 1: It shows that both variables are independent.\n",
    "\n",
    "Probability less than 0.05: It indicates that the relationship between the variables is significant at 95% confidence. The chi-square test statistic for a test of independence of two categorical variables is found by a formula\n",
    "\n",
    "**Categorical & Continuous:**\n",
    "- draw box plots for each level of categorical variables.\n",
    "    - If levels are small in number, it will not show the statistical significance. To look at the statistical significance we can perform Z-test, T-test or ANOVA.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8a9ac8",
   "metadata": {},
   "source": [
    "### Multivariate analysis\n",
    "```python\n",
    "sns.pairplot(df,hue=\"species\",size=3)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31818ae8",
   "metadata": {},
   "source": [
    "## Handling Missing Data\n",
    "### Identifying Missing Values\n",
    "Convert \"?\" to NaN and by isna method we can convert values to nan and check how many nan values are there in every column\n",
    "- Convert \"?\" to NaN. Or we can convert any missing value symbol to NaN with it\n",
    "    ```python\n",
    "    dataframe.replace(\"?\", np.nan, inplace = True)\n",
    "    ```\n",
    "- Check number of null values for every column\n",
    "    ```python\n",
    "    missing_values_count = dataframe.isnull().sum()\n",
    "    missing_values_count\n",
    "    ```\n",
    "- Check the number of total null values and find out the percentage it is taking\n",
    "    ```python\n",
    "    # how many total missing values do we have?\n",
    "    total_cells = np.product(dataframe.shape)\n",
    "    total_missing = missing_values_count.sum()\n",
    "\n",
    "    # percent of data that is missing\n",
    "    percent_missing = (total_missing/total_cells) * 100\n",
    "    print(percent_missing)\n",
    "    ```\n",
    "- if the proportion of column value missing is likely small enough for reasonable replacement with some form of imputation.if too much of that data missing to do something useful with at a basic level. We'll probably drop this later, or change it to another feature like \"col_name: 1 or 0\"\n",
    "    ```python\n",
    "    sns.heatmap(df.isnull(),yticklabels=False,cbar=False,cmap='viridis')\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a5438f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_missing_values(df):\n",
    "    df.replace(\"?\", np.nan, inplace = True)\n",
    "    \n",
    "    missing_values_count = df.isnull().sum()\n",
    "    print(missing_values_count)\n",
    "    \n",
    "    # how many total missing values do we have?\n",
    "    total_cells = np.product(df.shape)\n",
    "    total_missing = missing_values_count.sum()\n",
    "\n",
    "    # percent of data that is missing\n",
    "    percent_missing = (total_missing/total_cells) * 100\n",
    "    print(percent_missing)\n",
    "    \n",
    "    sns.heatmap(df.isnull(),yticklabels=False,cbar=False,cmap='viridis')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41825c0",
   "metadata": {},
   "source": [
    "### Dropping Missing Values\n",
    "with the dropna method, you can choose to drop rows or columns that contain missing values like NaN. Specify\n",
    "- axis = 0 to drop rows with missing value or,\n",
    "    ```python\n",
    "    dataframe.dropna(axis=0)\n",
    "    ```\n",
    "- axis = 1 to drop columns with missing value\n",
    "    ```python\n",
    "    dataframe.dropna(axis=1)\n",
    "    ```\n",
    "- to remove specific rows with the columns having missing values we can spepcify subset containing the column names and on dropna method we can specify axis = 0 to drop rows and inplace to True in order to modify directly to the dataframe\n",
    "    ```python\n",
    "    column_rows = ['column_name', 'column_name'....]\n",
    "    # To modify directly\n",
    "    datarame.dropna(subset=column_rows, axis = 0, inplace = True)\n",
    "    # Or to just save it to another dataframe\n",
    "    another_dataframe = datarame.dropna(subset=column_rows, axis = 0)\n",
    "    ```\n",
    "- To remove specify columns with missing values we can just drop the columns\n",
    "    ```python\n",
    "    column_to_drop = ['column_name', 'column_name'....]\n",
    "    dataframe.drop(columns=columns_to_drop, errors='ignore', inplace=True)\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c35f0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function drops missing values \n",
    "def drop_missing_values(df, drop_col, drop_row, row_to_drop, row_to_drop_by_col, col_to_drop, columns_to_drop):\n",
    "    \n",
    "    if drop_col:\n",
    "        df = df.dropna(axis=1)\n",
    "    if drop_row:\n",
    "        df = df.dropna(axis=0)\n",
    "    \n",
    "    if row_to_drop:\n",
    "        df = df.dropna(subset=row_to_drop_by_col, axis = 0)\n",
    "    if col_to_drop:\n",
    "        print(\"Columns in original dataset: %d \\n\" % df.shape[1])\n",
    "        df = df.drop(columns=columns_to_drop, errors='ignore')\n",
    "        # just how much data did we lose?\n",
    "        print(\"Columns with na's dropped: %d\" % df.shape[1])\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a241497",
   "metadata": {},
   "source": [
    "### Imputation according to outliers and normality\n",
    "- Use the check_outliers function to visualize outliers using box plots for each relevant numerical column.\n",
    "    -  Look for points beyond the \"whiskers\" of the box plot. These points are potential outliers.\n",
    "- Use the check_normality function to assess the normality of each numerical column using Q-Q plots and the Shapiro-Wilk test.\n",
    "    - **Q-Q Plot:** If the data points on the plot closely follow the diagonal line, the data is likely normally distributed.\n",
    "    - **Shapiro-Wilk Test:** A p-value > 0.05 indicates that the data is normally distributed.\n",
    "\n",
    "**Based on the results:**\n",
    "- If outliers are present and the data is not normally distributed, consider median imputation.\n",
    "- If outliers are minimal and the data is approximately normally distributed, mean imputation can be considered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3a777c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import shapiro\n",
    "from statsmodels.graphics.gofplots import qqplot\n",
    "\n",
    "# Function to check for outliers using box plots\n",
    "def check_outliers(df, column_name):\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    sns.boxplot(x=df[column_name])\n",
    "    plt.title(f'Boxplot for {column_name}')\n",
    "    plt.show()\n",
    "\n",
    "# Function to check for normality using Q-Q plot and Shapiro-Wilk test\n",
    "def check_normality(df, column_name):\n",
    "    # Q-Q plot\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    qqplot(df[column_name], line='s')\n",
    "    plt.title(f'Q-Q Plot for {column_name}')\n",
    "    plt.show()\n",
    "\n",
    "    # Shapiro-Wilk test for normality\n",
    "    stat, p_value = shapiro(df[column_name])\n",
    "    print(f'Shapiro-Wilk test for {column_name}:')\n",
    "    print(f'Statistic: {stat}, p-value: {p_value}')\n",
    "    if p_value > 0.05:\n",
    "        print(f'The data for {column_name} appears to be normally distributed.')\n",
    "    else:\n",
    "        print(f'The data for {column_name} does not appear to be normally distributed.')\n",
    "\n",
    "# Example usage for a specific column\n",
    "column_of_interest = ['columnn_names']\n",
    "\n",
    "for col in column_of_interest:\n",
    "    check_outliers(df, col)\n",
    "    check_normality(df, col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3568bc",
   "metadata": {},
   "source": [
    "### Replacing Missing Values\n",
    "method called replace which can be used to fill in the missing values with the newly calculated values.\n",
    "- Replace a column's missing values with a fixed value\n",
    "    ```python\n",
    "    dataframe[column_name] = dataframe[column_name].fillna(value)\n",
    "    ```\n",
    "- Replace a column's missing values with mean\n",
    "    ```python\n",
    "    # If the column is float type\n",
    "    mean = dataframe[column_name].mean()\n",
    "    # If the column is int type\n",
    "    mean = math.floor(dataframe[column_name].mean())\n",
    "    dataframe[column_name].replace(np.nan, mean, inplace=True)\n",
    "    ```\n",
    "- Similar case Imputation: In this case, we calculate average for some category mean individually of non missing values then replace the missing value based on category. For “Male“, we will replace missing values of manpower with 29.75 and for “Female” with 25.\n",
    "```python\n",
    "plt.figure(figsize=(12, 7))\n",
    "sns.boxplot(x='category_column_name',y='targetted_column',data=train,palette='winter')\n",
    "# If the result is valid we proceed with imputation\n",
    "def impute_targetted_col(cols):\n",
    "    targetted_column = cols[0]\n",
    "    category_column_name = cols[1]\n",
    "    \n",
    "    if pd.isnull(targetted_column):\n",
    "\n",
    "        if category_column_name == value:\n",
    "            return mean_of_that\n",
    "\n",
    "        elif category_column_name == value:\n",
    "            return mean_of_that\n",
    "\n",
    "        else:\n",
    "            return mean_of_that\n",
    "\n",
    "    else:\n",
    "        return Age\n",
    "train['targetted_column'] = train[['targetted_column','category_column_name']].apply(impute_age,axis=1)\n",
    "```\n",
    "- Replace a column's missing values with median\n",
    "    ```python\n",
    "    # If the column is float type\n",
    "    median = dataframe[column_name].median()\n",
    "    # If the column is int type\n",
    "    median = math.floor(dataframe[column_name].median())\n",
    "    dataframe[column_name].replace(np.nan, median, inplace=True)\n",
    "    ```\n",
    "- Replace a column's missing values with mode if categorical. We use [0] so that if there are multiple mode we take the first one\n",
    "    ```python\n",
    "    mode = dataframe[column_name].mode()[0]\n",
    "    dataframe[column_name].replace(np.nan, mode, inplace=True)\n",
    "    ```\n",
    "- Forward fill\n",
    "    ```python\n",
    "    dataframe[column_name].fillna(method='ffill', inplace=True)\n",
    "    ```\n",
    "- Backward fill\n",
    "    ```python\n",
    "    dataframe[column_name].fillna(method='bfill', inplace=True)\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e76ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_missing_values(df, fixed_col_nam, fixed_col_val, mean_col_nam, med_col_nam, mod_col_nam, ffill_col, bfill_col):\n",
    "    # Replace with Fixed value\n",
    "    for i, j in zip(fixed_col_nam, fixed_col_val):\n",
    "        df[i] = df[i].fillna(j)\n",
    "\n",
    "    # Replace with Mean value\n",
    "    for i in mean_col_nam:\n",
    "        mean = df[i].mean()\n",
    "        df[i].replace(np.nan, mean, inplace=True)\n",
    "    \n",
    "    # Replace with Median value\n",
    "    for i in med_col_nam:\n",
    "        median = df[i].median()\n",
    "        df[i].replace(np.nan, median, inplace=True)\n",
    "        \n",
    "    # Replace with Mode value\n",
    "    for i in mod_col_nam:\n",
    "        mode = df[i].mode()[0]\n",
    "        df[i].replace(np.nan, mode, inplace=True)\n",
    "        \n",
    "    # Forward fill\n",
    "    for i in ffill_col:\n",
    "        df[i].fillna(method='ffill', inplace=True)\n",
    "        \n",
    "    # Backward fill\n",
    "    for i in bfill_col:\n",
    "        df[i].fillna(method='bfill', inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01950cf7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a80bb081",
   "metadata": {},
   "source": [
    "### Replace missing values with prediction model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d4ff6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# Function to handle missing data using a prediction model\n",
    "def impute_missing_with_model(df, target_column):\n",
    "    # Split the dataset into two sets: one with no missing values and one with missing values\n",
    "    df_train = df.dropna(subset=[target_column])\n",
    "    df_test = df[df[target_column].isnull()]\n",
    "\n",
    "    # Identify predictor variables (features) and the target variable\n",
    "    X = df_train.drop(columns=[target_column])\n",
    "    y = df_train[target_column]\n",
    "\n",
    "    # Split the dataset into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Create a predictive model (you can use any model suitable for your data)\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Predict missing values\n",
    "    predictions = model.predict(df_test.drop(columns=[target_column]))\n",
    "\n",
    "    # Fill missing values in the original DataFrame\n",
    "    df.loc[df[target_column].isnull(), target_column] = predictions\n",
    "\n",
    "    # Evaluate the model performance (optional)\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    print(f'Mean Squared Error on the test set: {mse}')\n",
    "\n",
    "# Example usage for a specific column\n",
    "target_column_to_impute = 'your_target_column'\n",
    "impute_missing_with_model(df, target_column_to_impute)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325097bc",
   "metadata": {},
   "source": [
    "## Outlier detection\n",
    "- Outlier can be of two types: Univariate and Multivariate.\n",
    "- When we do the analysis we can detect the unusual points\n",
    "- the ideal way to tackle them is to find out the reason of having these outliers.\n",
    "- The method to deal with them would then depend on the reason of their occurrence.\n",
    "See type of error https://www.analyticsvidhya.com/blog/2016/01/guide-data-exploration/\n",
    "\n",
    "- To detect outliers use visualization methods Box-plot, Histogram, Scatter Plot\n",
    "\n",
    "    - Any value, which is beyond the range of -1.5 x IQR to 1.5 x IQR\n",
    "    - Use capping methods. Any value which out of range of 5th and 95th percentile can be considered as outlier\n",
    "    - Data points, three or more standard deviation away from mean are considered outlier\n",
    "    - Outlier detection is merely a special case of the examination of data for influential data points and it also depends on the business understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6a07c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import zscore\n",
    "import numpy as np\n",
    "\n",
    "# Load your dataset (replace 'your_data.csv' with your actual file)\n",
    "# df = pd.read_csv('your_data.csv')\n",
    "\n",
    "# Function to detect outliers using visualization methods\n",
    "def visualize_outliers(dataframe, column_name):\n",
    "    plt.figure(figsize=(15, 5))\n",
    "\n",
    "    # Box-plot\n",
    "    plt.subplot(1, 3, 1)\n",
    "    sns.boxplot(x=dataframe[column_name])\n",
    "    plt.title(f'Boxplot for {column_name}')\n",
    "\n",
    "    # Histogram\n",
    "    plt.subplot(1, 3, 2)\n",
    "    sns.histplot(dataframe[column_name], bins=30, kde=True)\n",
    "    plt.title(f'Histogram for {column_name}')\n",
    "\n",
    "    # Scatter Plot (for bivariate outlier detection)\n",
    "    plt.subplot(1, 3, 3)\n",
    "    sns.scatterplot(x=dataframe.index, y=dataframe[column_name])\n",
    "    plt.title(f'Scatter Plot for {column_name}')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Function to detect outliers using statistical measures\n",
    "def detect_outliers(dataframe, column_name):\n",
    "    # Z-Score method\n",
    "    z_scores = zscore(dataframe[column_name])\n",
    "    outliers_zscore = np.where(np.abs(z_scores) > 3)[0]\n",
    "\n",
    "    # Percentile Capping method\n",
    "    lower_limit = np.percentile(dataframe[column_name], 5)\n",
    "    upper_limit = np.percentile(dataframe[column_name], 95)\n",
    "    outliers_percentile = np.where((dataframe[column_name] < lower_limit) | (dataframe[column_name] > upper_limit))[0]\n",
    "\n",
    "    # Standard Deviation method\n",
    "    mean_value = np.mean(dataframe[column_name])\n",
    "    std_dev = np.std(dataframe[column_name])\n",
    "    outliers_std_dev = np.where(np.abs((dataframe[column_name] - mean_value) / std_dev) > 3)[0]\n",
    "\n",
    "    print(f'Outliers using Z-Score method: {outliers_zscore}')\n",
    "    print(f'Outliers using Percentile Capping method: {outliers_percentile}')\n",
    "    print(f'Outliers using Standard Deviation method: {outliers_std_dev}')\n",
    "\n",
    "# Example usage for a specific column\n",
    "column_of_interest = 'your_column_name'\n",
    "visualize_outliers(df, column_of_interest)\n",
    "detect_outliers(df, column_of_interest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d546985",
   "metadata": {},
   "source": [
    "### Outlier treatment\n",
    "**Delete Observation**\n",
    "- We delete outlier values if it is due to data entry error, data processing error or outlier observations are very small in numbers. \n",
    "- We can also use trimming at both ends to remove outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010f0f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import zscore\n",
    "import numpy as np\n",
    "\n",
    "# Load your dataset (replace 'your_data.csv' with your actual file)\n",
    "# df = pd.read_csv('your_data.csv')\n",
    "\n",
    "# Function to visualize and delete outliers using trimming\n",
    "def visualize_and_delete_outliers(dataframe, column_name, lower_percentile=5, upper_percentile=95):\n",
    "    plt.figure(figsize=(12, 4))\n",
    "\n",
    "    # Box-plot before trimming\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.boxplot(x=dataframe[column_name])\n",
    "    plt.title(f'Boxplot for {column_name} (Before Trimming)')\n",
    "\n",
    "    # Trimming outliers\n",
    "    lower_limit = np.percentile(dataframe[column_name], lower_percentile)\n",
    "    upper_limit = np.percentile(dataframe[column_name], upper_percentile)\n",
    "    trimmed_data = dataframe[(dataframe[column_name] >= lower_limit) & (dataframe[column_name] <= upper_limit)]\n",
    "\n",
    "    # Box-plot after trimming\n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.boxplot(x=trimmed_data[column_name])\n",
    "    plt.title(f'Boxplot for {column_name} (After Trimming)')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    return trimmed_data\n",
    "\n",
    "# Example usage for a specific column\n",
    "column_of_interest = 'your_column_name'\n",
    "trimmed_data = visualize_and_delete_outliers(df, column_of_interest)\n",
    "\n",
    "# Function to delete outliers using Z-Score\n",
    "def delete_outliers_zscore(dataframe, column_name, zscore_threshold=3):\n",
    "    z_scores = zscore(dataframe[column_name])\n",
    "    outliers_indices = np.where(np.abs(z_scores) > zscore_threshold)[0]\n",
    "    cleaned_data = dataframe.drop(index=outliers_indices)\n",
    "    return cleaned_data\n",
    "\n",
    "# Example usage for a specific column\n",
    "column_of_interest_zscore = 'your_column_name'\n",
    "cleaned_data_zscore = delete_outliers_zscore(df, column_of_interest_zscore)\n",
    "\n",
    "# Function to delete outliers using Percentile Capping\n",
    "def delete_outliers_percentile(dataframe, column_name, lower_percentile=5, upper_percentile=95):\n",
    "    lower_limit = np.percentile(dataframe[column_name], lower_percentile)\n",
    "    upper_limit = np.percentile(dataframe[column_name], upper_percentile)\n",
    "    cleaned_data = dataframe[(dataframe[column_name] >= lower_limit) & (dataframe[column_name] <= upper_limit)]\n",
    "    return cleaned_data\n",
    "\n",
    "# Example usage for a specific column\n",
    "column_of_interest_percentile = 'your_column_name'\n",
    "cleaned_data_percentile = delete_outliers_percentile(df, column_of_interest_percentile)\n",
    "\n",
    "# Function to delete outliers using Standard Deviation\n",
    "def delete_outliers_std_dev(dataframe, column_name, std_dev_threshold=3):\n",
    "    mean_value = np.mean(dataframe[column_name])\n",
    "    std_dev = np.std(dataframe[column_name])\n",
    "    outliers_indices = np.where(np.abs((dataframe[column_name] - mean_value) / std_dev) > std_dev_threshold)[0]\n",
    "    cleaned_data = dataframe.drop(index=outliers_indices)\n",
    "    return cleaned_data\n",
    "\n",
    "# Example usage for a specific column\n",
    "column_of_interest_std_dev = 'your_column_name'\n",
    "cleaned_data_std_dev = delete_outliers_std_dev(df, column_of_interest_std_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c772919",
   "metadata": {},
   "source": [
    "**Transform and Bin**\n",
    "- Natural log of a value reduces the variation caused by extreme values. \n",
    "- Binning is also a form of variable transformation. Decision Tree algorithm allows to deal with outliers well due to binning of variable. \n",
    "- We can also use the process of assigning weights to different observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e60c0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Load your dataset (replace 'your_data.csv' with your actual file)\n",
    "# df = pd.read_csv('your_data.csv')\n",
    "\n",
    "# List of columns of interest for separate techniques\n",
    "log_transform_columns = ['column1', 'column2', ...]\n",
    "binning_columns = ['column3', 'column4', ...]\n",
    "weighting_columns = ['column5', 'column6', ...]\n",
    "\n",
    "# Function to perform natural log transformation\n",
    "def log_transform(dataframe, column_name):\n",
    "    dataframe[f'log_{column_name}'] = np.log1p(dataframe[column_name])\n",
    "\n",
    "# Function to perform binning using Decision Tree algorithm\n",
    "def decision_tree_binning(dataframe, column_name):\n",
    "    # Use Decision Tree Regressor to create bins\n",
    "    model = DecisionTreeRegressor(max_leaf_nodes=10, random_state=42)\n",
    "    X = dataframe[column_name].values.reshape(-1, 1)\n",
    "    y = dataframe[column_name]\n",
    "    model.fit(X, y)\n",
    "\n",
    "    # Create a new column with bin labels\n",
    "    dataframe[f'bin_{column_name}'] = pd.cut(dataframe[column_name], bins=model.apply(X), include_lowest=True)\n",
    "\n",
    "# Function to assign weights to different observations\n",
    "def assign_weights(dataframe, column_name):\n",
    "    # Example: Assign higher weight to values above a certain threshold\n",
    "    threshold = 5\n",
    "    dataframe[f'weighted_{column_name}'] = np.where(dataframe[column_name] > threshold, dataframe[column_name] * 2, dataframe[column_name])\n",
    "\n",
    "# Example usage for a specific column\n",
    "for column in log_transform_columns:\n",
    "    log_transform(df, column)\n",
    "\n",
    "for column in binning_columns:\n",
    "    decision_tree_binning(df, column)\n",
    "\n",
    "for column in weighting_columns:\n",
    "    assign_weights(df, column)\n",
    "\n",
    "# Visualization for demonstration (you can adjust or add more as needed)\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Original vs. Log Transform\n",
    "plt.subplot(1, 3, 1)\n",
    "sns.histplot(df['column1'], kde=True, color='blue', label='Original')\n",
    "sns.histplot(df['log_column1'], kde=True, color='orange', label='Log Transform')\n",
    "plt.title('Log Transformation')\n",
    "plt.legend()\n",
    "\n",
    "# Original vs. Binning\n",
    "plt.subplot(1, 3, 2)\n",
    "sns.histplot(df['column3'], kde=True, color='blue', label='Original')\n",
    "sns.histplot(df['bin_column3'], kde=True, color='green', label='Binned')\n",
    "plt.title('Binning using Decision Tree')\n",
    "plt.legend()\n",
    "\n",
    "# Original vs. Weighted\n",
    "plt.subplot(1, 3, 3)\n",
    "sns.histplot(df['column5'], kde=True, color='blue', label='Original')\n",
    "sns.histplot(df['weighted_column5'], kde=True, color='red', label='Weighted')\n",
    "plt.title('Assigning Weights')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782bbb65",
   "metadata": {},
   "source": [
    "**Imputing**\n",
    "- If it is artificial, we can go with imputing values.\n",
    "- same as handling missing values\n",
    "\n",
    "**Treat Seperately**\n",
    "If there are significant number of outliers, we should treat them separately in the statistical model. One of the approach is to treat both groups as two different groups and build individual model for both groups and then combine the output.\n",
    "\n",
    "\n",
    "We can always compare with every technique and find the best possible matching to move on"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
