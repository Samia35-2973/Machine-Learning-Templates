{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4fb3c55",
   "metadata": {},
   "source": [
    "1. Convert data to numpy\n",
    "2. List out the models\n",
    "3. Cross Validate\n",
    "4. List out hyper paramaters\n",
    "5. Use list of headers and use it as index of the dictionary\n",
    "6. Perform grid search or randomized cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c52a559",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# importing the models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88826787",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.asarray(X)\n",
    "Y = np.asarray(Y)\n",
    "\n",
    "# list of models\n",
    "models = [LogisticRegression(max_iter=1000), SVC(kernel='linear'), KNeighborsClassifier(), RandomForestClassifier(random_state=0)]\n",
    "\n",
    "def compare_models_cross_validation():\n",
    "\n",
    "    for model in models:\n",
    "\n",
    "        cv_score = cross_val_score(model, X, Y, cv=5)\n",
    "        mean_accuracy = sum(cv_score)/len(cv_score)\n",
    "        mean_accuracy = mean_accuracy*100\n",
    "        mean_accuracy = round(mean_accuracy, 2)\n",
    "\n",
    "        print('Cross Validation accuracies for the',model,'=', cv_score)\n",
    "        print('Acccuracy score of the ',model,'=',mean_accuracy,'%')\n",
    "        print('---------------------------------------------------------------')\n",
    "        \n",
    "        \n",
    "compare_models_cross_validation()\n",
    "\n",
    "\n",
    "# list of models\n",
    "models_list = [LogisticRegression(max_iter=10000), SVC(), KNeighborsClassifier(), RandomForestClassifier(random_state=0)]\n",
    "\n",
    "# creating a dictionary that contains hyperparameter values for the above mentioned models\n",
    "\n",
    "\n",
    "model_hyperparameters = {\n",
    "    \n",
    "\n",
    "    'log_reg_hyperparameters': {\n",
    "        \n",
    "        'C' : [1,5,10,20]\n",
    "    },\n",
    "\n",
    "    'svc_hyperparameters': {\n",
    "        \n",
    "        'kernel' : ['linear','poly','rbf','sigmoid'],\n",
    "        'C' : [1,5,10,20]\n",
    "    },\n",
    "\n",
    "\n",
    "    'KNN_hyperparameters' : {\n",
    "        \n",
    "        'n_neighbors' : [3,5,10]\n",
    "    },\n",
    "\n",
    "\n",
    "    'random_forest_hyperparameters' : {\n",
    "        \n",
    "        'n_estimators' : [10, 20, 50, 100]\n",
    "    }\n",
    "}\n",
    "\n",
    "model_keys = list(model_hyperparameters.keys())\n",
    "print(model_keys)\n",
    "\n",
    "def ModelSelection(list_of_models, hyperparameters_dictionary):\n",
    "\n",
    "    result = []\n",
    "\n",
    "    i = 0\n",
    "\n",
    "    for model in list_of_models:\n",
    "\n",
    "        key = model_keys[i]\n",
    "\n",
    "        params = hyperparameters_dictionary[key]\n",
    "\n",
    "        i += 1\n",
    "\n",
    "        print(model)\n",
    "        print(params)\n",
    "        print('---------------------------------')\n",
    "\n",
    "\n",
    "        classifier = GridSearchCV(model, params, cv=5)\n",
    "\n",
    "        # fitting the data to classifier\n",
    "        classifier.fit(X,Y)\n",
    "\n",
    "        result.append({\n",
    "            'model used' : model,\n",
    "            'highest score' : classifier.best_score_,\n",
    "            'best hyperparameters' : classifier.best_params_\n",
    "        })\n",
    "\n",
    "        result_dataframe = pd.DataFrame(result, columns = ['model used','highest score','best hyperparameters'])\n",
    "\n",
    "    return result_dataframe\n",
    "\n",
    "ModelSelection(models_list, model_hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9d877b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best model\n",
    "best_model = Write_the_model_that_was_displayed\n",
    "\n",
    "# Predict using the best model\n",
    "Y_pred = best_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc486ea",
   "metadata": {},
   "source": [
    "## Accuracy for Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee8ec8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# accuracy on training data\n",
    "X_train_prediction = model.predict(X_train)\n",
    "training_data_accuracy = accuracy_score(Y_train, X_train_prediction)\n",
    "print(training_data_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043f9085",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "cm = confusion_matrix(Y_train, X_train_prediction)\n",
    "\n",
    "\n",
    "plt.figure(figsize = (10, 7))\n",
    "sb.heatmap(cm, annot = True)\n",
    "plt.xlabel(\"Predicted of training\")\n",
    "plt.ylabel(\"Y_train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9383294",
   "metadata": {},
   "source": [
    "## Accuracy for Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e507289",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation Metrics\n",
    "mse = mean_squared_error(Y_train, X_train_prediction)\n",
    "mae = mean_absolute_error(Y_train, X_train_prediction)\n",
    "r2 = r2_score(Y_train, X_train_prediction)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
    "print(f\"R-squared (R2): {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9fa048",
   "metadata": {},
   "source": [
    "## Precision, Recall and F1 Score\n",
    "- Precision is the ratio of number of True Positive to the total number of Predicted Positive. It measures, out of the total predicted positive, how many are actually positive.\n",
    "- Recall is the ratio of number of True Positive to the total number of Actual Positive. It measures, out of the total actual positive, how many are predicted as True Positive.\n",
    "- F1 Score is an important evaluation metric for binary classification that combines Precision & Recall. F1 Score is the harmonic mean of Precision & Recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b3dd8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_recall_f1_score(true_labels, pred_labels):\n",
    "\n",
    "    precision_value = precision_score(true_labels, pred_labels)\n",
    "    recall_value = recall_score(true_labels, pred_labels)\n",
    "    f1_score_value = f1_score(true_labels, pred_labels)\n",
    "\n",
    "    print('Precision =',precision_value)\n",
    "    print('Recall =',recall_value)\n",
    "    print('F1 Score =',f1_score_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1666afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification metrics for training data\n",
    "precision_recall_f1_score(Y_train, X_train_prediction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
